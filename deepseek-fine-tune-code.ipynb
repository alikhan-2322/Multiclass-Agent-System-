{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install unsloth datasets peft trl transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inspect the first few lines of the JSONL file\n",
    "!head -n 5 /kaggle/input/deepseek-fine-tune-v2/deepseek_finetune_dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"/kaggle/input/deepseek-fine-tune-v2/deepseek_finetune_dataset.jsonl\"\n",
    "output_file = \"/kaggle/working/deepseek_finetune_dataset_cleaned.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for i, line in enumerate(infile):\n",
    "        try:\n",
    "            # Replace single quotes with double quotes and parse\n",
    "            clean_line = line.replace(\"'\", '\"')\n",
    "            parsed = json.loads(clean_line)\n",
    "            json.dump(parsed, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping line {i} due to: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "input_file = \"/kaggle/input/deepseek-fine-tune-v2/deepseek_finetune_dataset.jsonl\"\n",
    "output_file = \"/kaggle/working/deepseek_finetune_dataset_cleaned.jsonl\"\n",
    "\n",
    "processed_lines_count = 0\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for i, line in enumerate(infile):\n",
    "        try:\n",
    "            clean_line = line.strip()\n",
    "            if not clean_line:\n",
    "                print(f\"Skipping empty line {i}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parsed = json.loads(clean_line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Try to parse with ast.literal_eval\n",
    "                parsed = ast.literal_eval(clean_line)\n",
    "\n",
    "            # Ensure string fields\n",
    "            for key in [\"output\", \"input\", \"instruction\", \"prompt\", \"completion\"]:\n",
    "                if key in parsed and not isinstance(parsed[key], str):\n",
    "                    parsed[key] = str(parsed[key])\n",
    "\n",
    "            json.dump(parsed, outfile, ensure_ascii=False)\n",
    "            outfile.write(\"\\n\")\n",
    "            processed_lines_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping line {i} due to: {e} - Content: {line.strip()[:100]}...\")\n",
    "\n",
    "print(f\"Finished cleaning. Successfully processed {processed_lines_count} lines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the cleaned JSONL dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/kaggle/working/deepseek_finetune_dataset_cleaned.jsonl\", split=\"train\")\n",
    "\n",
    "# Print column names\n",
    "print(dataset.column_names)\n",
    "\n",
    "# Print first sample\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"### Instruction:\\n{example['prompt']}\\n\\n### Response:\\n{example['completion']}\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(dataset[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "!pip install unsloth huggingface_hub\n",
    "\n",
    "# Import the necessary function and login\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "# Import Unsloth library\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load a model that's actually available in Unsloth format\n",
    "# Unsloth has optimized versions of models like Llama, Mistral, etc.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-2-7b-bnb-4bit\",  # This is a valid Unsloth model\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Get the PEFT model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Now the model is ready for fine-tuning or inference\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# If you specifically need DeepSeek Coder, you can try using the original model:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"  # or another DeepSeek model variant\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "model,\n",
    "r=32,\n",
    "target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "lora_alpha=32,\n",
    "random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T12:18:30.703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# First, inspect the dataset structure more thoroughly and print detailed info\n",
    "print(\"Dataset type:\", type(dataset))\n",
    "print(\"Dataset keys:\", dataset.features.keys())\n",
    "print(\"First example structure:\")\n",
    "first_example = dataset[0]\n",
    "for key, value in first_example.items():\n",
    "    print(f\"{key}: {type(value)} - {value}\")\n",
    "\n",
    "# Define a custom dataset class to ensure proper formatting\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=2048):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the raw text\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "\n",
    "        # Handle tokenization manually to ensure proper structure\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension that tokenizer adds by default when return_tensors=\"pt\"\n",
    "        input_ids = encodings[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encodings[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Create the labels (for causal LM, input_ids shifted right)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Create compute metrics function\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Flatten for token-level tasks\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "\n",
    "    # Remove ignored index (-100)\n",
    "    mask = labels != -100\n",
    "    predictions = predictions[mask]\n",
    "    labels = labels[mask]\n",
    "\n",
    "    # Handle empty cases\n",
    "    if len(labels) == 0:\n",
    "        return {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro', zero_division=0),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro', zero_division=0),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro', zero_division=0),\n",
    "    }\n",
    "\n",
    "# Ensure tokenizer is properly configured\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Split dataset\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_raw = split_dataset[\"train\"]\n",
    "eval_raw = split_dataset[\"test\"]\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(train_raw, tokenizer)\n",
    "eval_dataset = CustomDataset(eval_raw, tokenizer)\n",
    "\n",
    "# Print an example of processed data to verify structure\n",
    "print(\"\\nProcessed example structure:\")\n",
    "processed_example = train_dataset[0]\n",
    "for key, value in processed_example.items():\n",
    "    print(f\"{key}: {type(value)} - Shape: {value.shape}\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"deepseek_v2_finetuned\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer with our custom dataset\n",
    "print(\"\\nInitializing trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # No need for dataset_text_field since we're providing processed data\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7449735,
     "sourceId": 11855990,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
